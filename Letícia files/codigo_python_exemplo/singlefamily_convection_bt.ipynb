{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'D:/OneDrive - UFSC/Artigos/2023_Thesis/simulacoes/saopaulo/U001_Caso01_1a7_ac_1.csv'\n",
    "df_origin = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_origin_origin)\n",
    "\n",
    "# read the columns name to know which variable is needed\n",
    "''' This way returns a data.frame with 5 row x number of columns\n",
    "data_colnames = df.head()\n",
    "\n",
    "data_colnames \n",
    "'''\n",
    "''' this ways returns a list with all columns names '''\n",
    "\n",
    "data_colnames = list(df_origin.columns)\n",
    "\n",
    "# data_colnames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with all outputs for each prolonged stay room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sala = []\n",
    "\n",
    "df_sala = df.filter(regex = r'(SALA)')\n",
    "\n",
    "# for dorm1 and dorm2 df, it is needed call doors outputs too\n",
    "\n",
    "df_dorm1 = df.filter(regex = r'(DORM1)') + df.filter(regex = r'(SALA_PORTAIN_0_01D)')\n",
    "\n",
    "df_dorm2 = df.filter(regex = r'(DORM1)') + df.filter(regex = r'(SALA_PORTAIN_0_02D)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the convective thermal balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convection_living = pd.DataFrame(columns=['date','Tair','Top','internal_gains','int_walls','N_walls','S_walls','W_walls','E_walls','floor','roof','N_window','S_window','W_window','E_window','NVout_gains','NVout_losses','NVzone_gains','NV_losses','cooling','heating'])\n",
    "\n",
    "date = df.filter(regex = r'(Date)')\n",
    "dbt = df.filter(regex = r'(Drybulb)')\n",
    "top = df.filter(regex = r'(SALA:Zone Operative)')\n",
    "internal_gains = df.filter(regex = r'(SALA:Zone Total Internal)')\n",
    "int_walls = df.filter(regex = r'(SALA_PARIN)') + df.filter(regex = r'(SALA_PORTAIN)')\n",
    "N_walls = [df.filter(regex = r'(SALA_PAREX_00S)'), df.filter(regex = r'(SALA_PORTAEX_0_00S)')]\n",
    "S_walls = df.filter(regex = r'(SALA_PAREX_00I)')\n",
    "E_walls = [df.filter(regex = r'(SALA_PAREX_00D)'), df.filter(regex = r'(SALA_PORTAEX_0_00D)')]\n",
    "W_walls = df.filter(regex = r'(SALA_PAREX_01E)')\n",
    "S_window = df.filter(regex = r'(SALA_JAN_0_00I)')\n",
    "W_window = df.filter(regex = r'(SALA_JAN_0_01E)')\n",
    "floor = df.filter(regex = r'(SALA_PISO)')\n",
    "roof = df.filter(regex = r'(SALA_COB)')\n",
    "\n",
    "# convection_living = pd.DataFrame()\n",
    "\n",
    "#tentar usar o processo de series para criar o df como eu quero\n",
    "# int_teste = pd.DataFrame(list(int_walls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {'name' :['date','dbt','top','internal_gains','int_walls','int_doors','N_walls','N_doors','S_walls','E_walls','E_doors','W_walls','S_window','W_window','floor','roof'],\n",
    "'csv_columns' :['Date','Drybulb','SALA:Zone Operative','SALA:Zone Total Internal','SALA_PARIN','SALA_PORTAIN','SALA_PAREX_00S','SALA_PORTAEX_0_00S','SALA_PAREX_00I',\n",
    "'SALA_PAREX_00D','SALA_PORTAEX_0_00D','SALA_PAREX_01E','SALA_JAN_0_00I','SALA_JAN_0_01E','SALA_PISO','SALA_COB']}\n",
    "\n",
    "df_names = pd.DataFrame(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_names\n",
    "''' eu quero criar um for para fazer uma iteração entre as colunas do csv e para ele ler quais precisam ser somadas ou não'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['date','dbt','top','internal_gains','int_walls','N_walls','N_doors','S_walls','E_walls','E_doors','W_walls','S_window','W_window','floor','roof']\n",
    "\n",
    "names = ['date' : 'Date', 'dbt' = 'Drybulb']\n",
    "\n",
    "int_walls = \n",
    "\n",
    "series = {'date' : df_origin.filter(regex = r'(Date)'),\n",
    "    'dbt': df_origin.filter(regex = r'(Drybulb)'),\n",
    "    'top' : df_origin.filter(regex = r'(SALA:Zone Operative)'),\n",
    "    'internal_gains' : df_origin.filter(regex = r'(SALA:Zone Total Internal)'),\n",
    "    'int_walls' : [if (df_origin.filter(regex = r'(SALA_PARIN)').shape[1] > 1) :\n",
    "            (df_origin.filter(regex = r'(SALA_PARIN)')).sum(axis= 1)\n",
    "        else:\n",
    "        df_origin.filter(regex = r'(SALA_PARIN)')]\n",
    "    'int_doors' : df_origin.filter(regex = r'(SALA_PORTAIN)'),\n",
    "    'N_walls' : df_origin.filter(regex = r'(SALA_PAREX_00S)'), \n",
    "    'N_doors' : df_origin.filter(regex = r'(SALA_PORTAEX_0_00S)'),\n",
    "    'S_walls' : df_origin.filter(regex = r'(SALA_PAREX_00I)'),\n",
    "    'E_walls' : df_origin.filter(regex = r'(SALA_PAREX_00D)'),\n",
    "    'E_doors' :  df_origin.filter(regex = r'(SALA_PORTAEX_0_00D)'),\n",
    "    'W_walls' : df_origin.filter(regex = r'(SALA_PAREX_01E)'),\n",
    "    'S_window' : df_origin.filter(regex = r'(SALA_JAN_0_00I)'),\n",
    "    'W_window' : df_origin.filter(regex = r'(SALA_JAN_0_01E)'),\n",
    "    'floor' : df_origin.filter(regex = r'(SALA_PISO)'),\n",
    "    'roof' : df_origin.filter(regex = r'(SALA_COB)')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df_origin.filter(regex = r'(SALA_PARIN)').shape[1] > 1) :\n",
    "        int_walls : (df_origin.filter(regex = r'(SALA_PARIN)')).sum(axis= 1)\n",
    "else:\n",
    "        int_walls : df_origin.filter(regex = r'(SALA_PARIN)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (df_origin.filter(regex = r'(SALA_PARIN)')).sum(axis= 1)\n",
    "b = df_origin.filter(regex = r'(SALA_PARIN)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "[series[key] for key in series.keys()] me resulta no nome de cada dataframe dentro de series\n",
    "axis = 1 siginifica que eu quero unir os dataframes por colunas\n",
    "ignore_index = True significa que eu não quero que apareça o index no meu df final\n",
    "'''\n",
    "\n",
    "df2 = pd.concat([series[key] for key in series.keys()],axis=1, keys = [series[key] for key in series.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [key for key in series.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'roof'\n",
    "series[key].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btsurface(dataframe):\n",
    "    df = dataframe\n",
    "    for i  in \n",
    "    if (list(dataframe.columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trocas = []\n",
    "\n",
    "trocas = pd.DataFrame([series])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bttotal = []\n",
    "flux = []\n",
    "\n",
    "bttotal = pd.DataFrame([flux.append(trocas)])\n",
    "\n",
    "bttotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bttotal = []\n",
    "\n",
    "bttotal = pd.DataFrame(trocas)\n",
    "\n",
    "bttotal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo do próximo bloco pode ser repetido para a função de btconvecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btsurface(dataframe):\n",
    "    df = dataframe\n",
    "    dfs =[]\n",
    "    #Aqui tu vai colocar os filtros de acordo com a tua necessidade\n",
    "    series = {'date' : df.filter(regex = r'(Date)'),\n",
    "    'dbt': df.filter(regex = r'(Drybulb)'),\n",
    "    'top' : df.filter(regex = r'(SALA:Zone Operative)'),\n",
    "    'internal_gains' : df.filter(regex = r'(SALA:Zone Total Internal)'),\n",
    "    'int_walls' : df.filter(regex = r'(SALA_PARIN)'),\n",
    "    'int_doors' : df.filter(regex = r'(SALA_PORTAIN)'),\n",
    "    'N_walls' : df.filter(regex = r'(SALA_PAREX_00S)'), \n",
    "    'N_doors' : df.filter(regex = r'(SALA_PORTAEX_0_00S)'),\n",
    "    'S_walls' : df.filter(regex = r'(SALA_PAREX_00I)'),\n",
    "    'E_walls' : df.filter(regex = r'(SALA_PAREX_00D)'),\n",
    "    'E_doors' :  df.filter(regex = r'(SALA_PORTAEX_0_00D)'),\n",
    "    'W_walls' : df.filter(regex = r'(SALA_PAREX_01E)'),\n",
    "    'S_window' : df.filter(regex = r'(SALA_JAN_0_00I)'),\n",
    "    'W_window' : df.filter(regex = r'(SALA_JAN_0_01E)'),\n",
    "    'floor' : df.filter(regex = r'(SALA_PISO)'),\n",
    "    'roof' : df.filter(regex = r'(SALA_COB)')}\n",
    "\n",
    "    bttotal_final = []\n",
    "    bttotal = []\n",
    "    trocas = pd.Series(series)\n",
    "    # bttotal_final.append(trocas)\n",
    "    bttotal.append(trocas)\n",
    "    bttotal_final = pd.DataFrame([bttotal])\n",
    "\n",
    "    #bttotal = (series)\n",
    "   # for serie in series:\n",
    "    #    bttotal_final = []\n",
    "        #Vai somar os resultados das colunas para te dar o balanço\n",
    "     #   bttotal = pd.DataFrame(series)\n",
    "        #bttotal_final.append(bttotal)\n",
    "        #df_bt_surface = pd.DataFrame([bttotal_final])\n",
    "\n",
    "    return bttotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = btsurface(df_origin)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btsurface(dataframe):\n",
    "    df = dataframe\n",
    "    dfs =[]\n",
    "    #Aqui tu vai colocar os filtros de acordo com a tua necessidade\n",
    "    series = [df.filter(regex = r'(Sala)'),\n",
    "    df.filter(regex = r'(Electric Equipment)'),\n",
    "    df.filter(regex = r'(Fan Electric Energy)'),\n",
    "    df.filter(regex = r'(PTHP COOLING COIL)'),\n",
    "    df.filter(regex = r'(PTHP HEATING COIL)'),\n",
    "    df.filter(regex = r'(VRF Heat Pump Cooling Electric Energy)'),\n",
    "    df.filter(regex = r'(VRF Heat Pump Heating Electric Energy)'),\n",
    "    df.filter(regex = r'(VRF Heat Pump Defrost Electric Energy)'),\n",
    "    df.filter(regex = r'(VRF Heat Pump Crankcase Heater Electric Energy)')]   \n",
    "\n",
    "    for serie in series:\n",
    "        bttotal_final = []\n",
    "        #Vai somar os resultados das colunas para te dar o balanço\n",
    "        bttotal = pd.Series(serie).sum()\n",
    "        bttotal_final.append(bttotal)\n",
    "        df_bt_surface = pd.DataFrame([bttotal_final])\n",
    "        #Se tu quiser rodar para todos os csvs, e juntar todos os resultados em um só, executa essa linha de baixo e a linha do df_final = pd.concat...\n",
    "        #dfs.append(df_bt_surface)\n",
    "\n",
    "    #df_final= pd.concat(dfs,axis = 1)\n",
    "    #return df_final #Caso tu queira executar uma função que vai rodar em todos os cvs de todos os teus casos e retornar um csv com TODOS os casos\n",
    "    return df_bt_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui tu define uma tag, que vai identificar a orientação que tu quer.\n",
    "#Outra forma de fazer é ter uma função para cada orientação, e ai ao invés de tu informar a tag, tu chama funções diferentes cada hora que tu quer.\n",
    "#Isso depende do que tu achar melhor.\n",
    "\n",
    "def btsurfaceori(df_bt_surface, tag):\n",
    "    df.filter(tag) #tem que ver se assim vai dar certo. Mas é algo desse jeito assim.\n",
    "\n",
    "    #define as operações que tu quer fazer. Lembra que o dataframe que entra aqui é o que ja teve os dados processados\n",
    "    #ou seja, ja foram somadas as colunas\n",
    "    df_tratado = df.filter(tag)\n",
    "    return df_tratado\n",
    "\n",
    "#O mesmo pode ser feito para a convecção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A parte do SQL ou do HTML tu precisa ler o arquivo ou em HTML, ou SQL e encontrar a tabela que tem as superfícies e a orientação dela\n",
    "#Ai tu pode fazer com que ele organize e agrupe as superfícies que estão no norte em uma lista, ou em um dicionário. E depois isso tu pode usar para tratar os dados\n",
    "#por exmeplo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8344beb2e3695cc865366b87c9e795fe295d5e1a75013da0c7029a373688a8d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
